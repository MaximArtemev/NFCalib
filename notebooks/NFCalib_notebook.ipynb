{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'MINIBOONE'\n",
    "model_type = 'MAF'\n",
    "\n",
    "num_layers = 5\n",
    "num_iters = int(3 * 1e6)\n",
    "\n",
    "model_name = f\"{model_type}_{num_layers}\"\n",
    "\n",
    "model_save_dir = f'../models/{data_name}/{model_name}/'\n",
    "os.makedirs(f'{model_save_dir}/checkpoints', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils import data_utils\n",
    "\n",
    "data_mapping = {'BSDS300': data_utils.BSDS300,\n",
    "                'GAS': data_utils.GAS,\n",
    "                'MINIBOONE': data_utils.MINIBOONE,\n",
    "                'POWER': data_utils.POWER,\n",
    "                'HEPMASS': data_utils.HEPMASS}\n",
    "data = data_mapping[data_name]()\n",
    "dim = data.n_dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(data.trn.x).to(device)\n",
    "X_test_tensor = torch.from_numpy(data.tst.x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29556, 43])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "prior = MultivariateNormal(torch.zeros(data.n_dims).to(device),\n",
    "                           torch.eye(data.n_dims).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mrartemev_nflib.flows import NormalizingFlowModel, InvertiblePermutation, Invertible1x1Conv, ActNorm, NSF_AR\n",
    "from src.mrartemev_nflib.flows import MAF, AffineHalfFlow\n",
    "from src.mrartemev_nflib.nn import ARMLP, MLP\n",
    "\n",
    "\n",
    "flows = []\n",
    "for _ in range(num_layers):\n",
    "    if model_type == 'MAF':\n",
    "        flows.append(MAF(dim=data.n_dims, base_network=ARMLP))\n",
    "        flows.append(InvertiblePermutation(dim=data.n_dims))\n",
    "    if model_type == 'SPLINE-AR':\n",
    "        flows.append(ActNorm(dim=data.n_dims))\n",
    "        flows.append(Invertible1x1Conv(dim=data.n_dims))\n",
    "        flows.append(NSF_AR(dim=data.n_dims, K=8, B=3, hidden_features=32, depth=1, base_network=MLP))\n",
    "    if model_type == 'GLOW':\n",
    "        flows.append(ActNorm(dim=data.n_dims))\n",
    "        flows.append(Invertible1x1Conv(dim=data.n_dims))\n",
    "        flows.append(AffineHalfFlow(dim=data.n_dims, hidden_features=32, base_network=MLP))\n",
    "        flows.append(InvertiblePermutation(dim=data.n_dims))\n",
    "    if model_type == 'RealNVP':\n",
    "        flows.append(AffineHalfFlow(dim=data.ndims))\n",
    "        flows.append(InvertiblePermutation(dim=data.n_dims))\n",
    "\n",
    "lr = 0.0005\n",
    "        \n",
    "model = NormalizingFlowModel(prior, flows).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(model, device):\n",
    "    model.to(device)\n",
    "    model.prior = MultivariateNormal(torch.zeros(data.n_dims).to(device),\n",
    "                                     torch.eye(data.n_dims).to(device))\n",
    "\n",
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from itertools import repeat\n",
    "\n",
    "dloader = DataLoader(TensorDataset(X_train_tensor), batch_size=14000,\n",
    "                     shuffle=True, drop_last=True)\n",
    "test_dloader = DataLoader(TensorDataset(X_test_tensor), batch_size=2**8,\n",
    "                          shuffle=True, drop_last=True)\n",
    "\n",
    "def repeater(data_loader):\n",
    "    for loader in repeat(data_loader):\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "dloader = repeater(dloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "checkpoint_frequency = 200000\n",
    "\n",
    "epoch_logpx = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:35<00:00, 27.97it/s]\n"
     ]
    }
   ],
   "source": [
    "for iteration in tqdm(range(len(epoch_logpx), num_iters), position=0):\n",
    "    batch = next(dloader)[0]\n",
    "    model.train()\n",
    "\n",
    "    # fit\n",
    "    optimizer.zero_grad()\n",
    "    logp_x = model.log_prob(batch)\n",
    "    loss = -torch.mean(logp_x)\n",
    "    loss.backward()\n",
    "\n",
    "    # clip\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "    # step\n",
    "    optimizer.step()\n",
    "\n",
    "    # log\n",
    "    epoch_logpx.append(-loss.item())\n",
    "    \n",
    "\n",
    "        \n",
    "    # eval & save\n",
    "    if iteration % checkpoint_frequency == 0 and iteration != 0:     \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            eval_logp_x = np.mean([torch.mean(model.log_prob(batch[0].to(device))).item() for batch in test_dloader])\n",
    "            train_logp_x = np.mean(epoch_logpx[-checkpoint_frequency:])\n",
    "            print(f\" {iteration}/{num_iters}, val logpx: {eval_logp_x}, train logpx: {train_logp_x}\")\n",
    "            torch.save({'iteration': iteration,\n",
    "                        'model.state_dict()': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'val_logp_x': eval_logp_x,\n",
    "                        'train_logp_x': eval_logp_x\n",
    "                       }, os.path.join(model_save_dir, 'checkpoints', f'{iteration}.checkpoint'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_logp_x = np.mean([torch.mean(model.log_prob(batch[0].to(device))).item() for batch in test_dloader])\n",
    "train_logp_x = np.mean(epoch_logpx[-checkpoint_frequency:])\n",
    "\n",
    "torch.save({'iteration': iteration,\n",
    "            'model.state_dict()': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_logp_x': eval_logp_x,\n",
    "            'train_logp_x': eval_logp_x\n",
    "           }, os.path.join(model_save_dir, 'final_model.checkpoint'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from src.nf import CalibratedModel, neg_log_likelihood\n",
    "from src.nf.classifiers import train_catboost_clf\n",
    "from scipy.special import logsumexp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_sample(model, n, batch_size=2**8):\n",
    "    generated = []\n",
    "    for _ in range(n // batch_size):\n",
    "        generated_batch = model.sample(batch_size)\n",
    "        generated.append(generated_batch.cpu().detach())\n",
    "    if n % batch_size != 0:\n",
    "        generated_batch = model.sample(n % batch_size)\n",
    "        generated.append(generated_batch.cpu().detach())\n",
    "    generated = torch.cat(generated, dim=0)\n",
    "    assert n == len(generated)\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.sample_n = lambda n: batched_sample(model, n)\n",
    "\n",
    "to_device(model, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test LL  76.87403869628906\n"
     ]
    }
   ],
   "source": [
    "n = min(100000, X_test_tensor.shape[0])\n",
    "\n",
    "print('Model test LL ', torch.mean(model.log_prob(X_test_tensor[:n].cpu())).item())\n",
    "\n",
    "clf_ds_train = np.row_stack([\n",
    "    np.column_stack([X_train_tensor[:n].cpu().detach().numpy(), np.ones(n).reshape(-1, 1)]),\n",
    "    np.column_stack([model.sample_n(n).cpu().detach().numpy(), np.zeros(n).reshape(-1, 1)])\n",
    "]).astype(np.float32)\n",
    "\n",
    "clf_ds_test = np.row_stack([\n",
    "    np.column_stack([X_test_tensor[:n].cpu().detach().numpy(), np.ones(n).reshape(-1, 1)]),\n",
    "    np.column_stack([model.sample_n(n).cpu().detach().numpy(), np.zeros(n).reshape(-1, 1)])\n",
    "]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.058239\n",
      "0:\ttest: 0.8470448\tbest: 0.8470448 (0)\ttotal: 107ms\tremaining: 8m 53s\n",
      "1000:\ttest: 0.9992977\tbest: 0.9992977 (1000)\ttotal: 28.6s\tremaining: 1m 54s\n",
      "2000:\ttest: 0.9993983\tbest: 0.9993983 (2000)\ttotal: 56.4s\tremaining: 1m 24s\n",
      "3000:\ttest: 0.9994075\tbest: 0.9994075 (3000)\ttotal: 1m 21s\tremaining: 54.3s\n",
      "4000:\ttest: 0.9994107\tbest: 0.9994107 (4000)\ttotal: 1m 45s\tremaining: 26.4s\n",
      "4999:\ttest: 0.9994140\tbest: 0.9994140 (4999)\ttotal: 2m 10s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9994139532\n",
      "bestIteration = 4999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = CatBoostClassifier(\n",
    "    5000, eval_metric='AUC',\n",
    "    metric_period=1000,\n",
    ").fit(\n",
    "    clf_ds_train[:, :-1], clf_ds_train[:, -1],\n",
    "    eval_set=(clf_ds_test[:, :-1], clf_ds_test[:, -1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76.81639099121094, 92.95306469249624)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibrated_model = CalibratedModel(clf, model, logit=True)\n",
    "\n",
    "samples = model.sample_n(n).cpu().detach().cpu().numpy()\n",
    "clf_preds = clf.predict(samples, prediction_type='RawFormulaVal')\n",
    "calibration_constant = logsumexp(clf_preds) - np.log(len(clf_preds))\n",
    "\n",
    "-neg_log_likelihood(model, X_test_tensor.cpu().detach()), \\\n",
    "-neg_log_likelihood(calibrated_model, X_test_tensor.cpu().detach()) - calibration_constant,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
